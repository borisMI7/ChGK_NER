{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe7c481",
   "metadata": {},
   "source": [
    "---\n",
    "# –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "\n",
    "> **üéØ –¶–µ–ª—å —ç—Ç–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞:** –ø—Ä–æ–≤–µ—Å—Ç–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ —Å–æ–±—Ä–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –ø—Ä–∏ –ø–æ–º–æ—â–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–π NER –º–æ–¥–µ–ª–∏.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354ae8c",
   "metadata": {},
   "source": [
    "## –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29010531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc83d90",
   "metadata": {},
   "source": [
    "## –ò–Ω—Ñ–µ—Ä–µ–Ω—Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf5865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DATA_PATH = \"../data/got_q_db.parquet\"\n",
    "OUTPUT_DATA_PATH = \"../data/chgk_database_with_entities.parquet\"\n",
    "MODEL_NAME = \"borisMI/ChGK_NER\"\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "if device == 0:\n",
    "    print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU. –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.\")\n",
    "\n",
    "print(f\"\\n–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {SOURCE_DATA_PATH}...\")\n",
    "df = pd.read_parquet(SOURCE_DATA_PATH)\n",
    "print(\"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.\")\n",
    "\n",
    "print(\"\\n–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ 'full_text'...\")\n",
    "df['question'] = df['text'].fillna('')\n",
    "df['answer'] = df['answer'].fillna('')\n",
    "df['comments'] = df['comment'].fillna('')\n",
    "df['full_text'] = df['question'] + ' ' + df['answer'] + ' ' + df['comments']\n",
    "print(\"–ö–æ–ª–æ–Ω–∫–∞ 'full_text' —Å–æ–∑–¥–∞–Ω–∞.\")\n",
    "\n",
    "\n",
    "print(f\"\\n–ó–∞–≥—Ä—É–∑–∫–∞ NER-–º–æ–¥–µ–ª–∏ '{MODEL_NAME}'...\")\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=MODEL_NAME,\n",
    "    tokenizer=MODEL_NAME,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=device\n",
    ")\n",
    "print(\"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.\")\n",
    "tokenizer = ner_pipeline.tokenizer\n",
    "max_length = 512\n",
    "\n",
    "texts_to_process = df['full_text'].tolist()\n",
    "\n",
    "print(f\"\\n–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –æ–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã ({max_length} —Ç–æ–∫–µ–Ω–æ–≤)...\")\n",
    "truncated_texts = []\n",
    "for text in tqdm(texts_to_process, desc=\"–û–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\"):\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_length)\n",
    "    truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    truncated_texts.append(truncated_text)\n",
    "\n",
    "print(\"–û–±—Ä–µ–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2712ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n–ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π. –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(truncated_texts)}.\")\n",
    "\n",
    "all_entities = []\n",
    "for i in tqdm(range(0, len(truncated_texts), BATCH_SIZE), desc=\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π\"):\n",
    "\n",
    "    batch_texts = truncated_texts[i:i + BATCH_SIZE]\n",
    "    results = ner_pipeline(batch_texts)\n",
    "    all_entities.extend(results)\n",
    "\n",
    "print(\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ.\")\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame\n",
    "df['entities'] = all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_columns = ['id', 'author', 'date', 'entities']\n",
    "\n",
    "existing_context_columns = [col for col in context_columns if col in df.columns]\n",
    "print(f\"\\n–í—ã–±–∏—Ä–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {existing_context_columns}\")\n",
    "\n",
    "df_subset = df[existing_context_columns]\n",
    "df_exploded = df_subset.explode('entities').dropna(subset=['entities']).reset_index(drop=True)\n",
    "entity_details = pd.json_normalize(df_exploded['entities'])\n",
    "df_final = df_exploded.join(entity_details)\n",
    "df_final = df_final.drop(columns=['entities'])\n",
    "\n",
    "if 'playedAt' in df_final.columns:\n",
    "    df_final['year'] = pd.to_datetime(df_final['playedAt'], errors='coerce').dt.year\n",
    "\n",
    "print(\"\\n–ò—Ç–æ–≥–æ–≤—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame (–∫–∞–∂–¥–∞—è —Å—É—â–Ω–æ—Å—Ç—å –≤ —Å–≤–æ–µ–π —Å—Ç—Ä–æ–∫–µ):\")\n",
    "print(df_final.info())\n",
    "print(df_final.head())\n",
    "\n",
    "OUTPUT_FILE = \"chgk_entities_structured.parquet\"\n",
    "df_final.to_parquet(OUTPUT_FILE, index=False)\n",
    "print(f\"\\n–ß–∏—Å—Ç—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
