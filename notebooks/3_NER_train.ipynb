{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc8f5d2",
   "metadata": {},
   "source": [
    "---\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "\n",
    "> **üéØ –¶–µ–ª—å —ç—Ç–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞:** –æ–±—É—á–∏—Ç—å –∫–∞—Å—Ç–æ–º–Ω—É—é NER –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ ai-forever/ruBert-large –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ –≤–æ–ø—Ä–æ—Å–∞—Ö –ß–ì–ö.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a210821",
   "metadata": {},
   "source": [
    "*   **`AUTHOR`**: –°–æ–∑–¥–∞—Ç–µ–ª–∏ (–ü—É—à–∫–∏–Ω, –¢–∞—Ä–∞–Ω—Ç–∏–Ω–æ).\n",
    "*   **`CHARACTER`**: –ü–µ—Ä—Å–æ–Ω–∞–∂–∏ (–û–Ω–µ–≥–∏–Ω, –ó–µ–≤—Å).\n",
    "*   **`PERSON`**: –†–µ–∞–ª—å–Ω—ã–µ –ª—é–¥–∏, –Ω–µ —è–≤–ª—è—é—â–∏–µ—Å—è –∞–≤—Ç–æ—Ä–∞–º–∏ –≤ –¥–∞–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (–ù–∞–ø–æ–ª–µ–æ–Ω –∫–∞–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è —Ñ–∏–≥—É—Ä–∞, –Æ—Ä–∏–π –ì–∞–≥–∞—Ä–∏–Ω).\n",
    "*   **`WORK_OF_ART`**: –ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è (\"–ï–≤–≥–µ–Ω–∏–π –û–Ω–µ–≥–∏–Ω\", \"–ö—Ä–∏–º–∏–Ω–∞–ª—å–Ω–æ–µ —á—Ç–∏–≤–æ\").\n",
    "*   **`LOCATION`**: –ú–µ—Å—Ç–∞ (–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –õ—É–≤—Ä)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be5419",
   "metadata": {},
   "source": [
    "## –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739954bd",
   "metadata": {},
   "source": [
    "## –†–∞–∑–±–∏–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏ –Ω–∞ 3 —á–∞—Å—Ç–∏ - —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é, –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ad66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/annotated_data.json', 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "\n",
    "train_data, temp_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "with open('train_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open('val_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open('test_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã:\")\n",
    "print(f\"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ train_data.json)\")\n",
    "print(f\"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ val_data.json)\")\n",
    "print(f\"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ test_data.json)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb27029",
   "metadata": {},
   "source": [
    "## –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa44a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"ai-forever/ruBert-large\"\n",
    "OUTPUT_DIR = \"./my-chgk-ner-model-v1\"\n",
    "TRAIN_FILE = \"train_data.json\"\n",
    "TEST_FILE = \"val_data.json\"\n",
    "\n",
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "\n",
    "dataset = load_dataset('json', data_files={'train': TRAIN_FILE, 'test': TEST_FILE})\n",
    "print(\"\\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:\")\n",
    "print(dataset)\n",
    "\n",
    "labels_from_data = set()\n",
    "for item in dataset['train']['annotations']:\n",
    "    for entity in item[0]['result']:\n",
    "        labels_from_data.add(entity['value']['labels'][0])\n",
    "\n",
    "unique_labels = sorted(list(labels_from_data))\n",
    "label2id = {}\n",
    "id2label = {}\n",
    "\n",
    "for label in unique_labels:\n",
    "    label2id[f\"B-{label}\"] = len(label2id)\n",
    "    id2label[len(id2label)] = f\"B-{label}\"\n",
    "    label2id[f\"I-{label}\"] = len(label2id)\n",
    "    id2label[len(id2label)] = f\"I-{label}\"\n",
    "\n",
    "label2id[\"O\"] = len(label2id)\n",
    "id2label[len(id2label)] = \"O\"\n",
    "\n",
    "print(\"\\n–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏:\")\n",
    "print(id2label)\n",
    "\n",
    "print(\"\\n--- –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ---\")\n",
    "start_time = time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "end_time = time.time()\n",
    "print(f\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {end_time - start_time:.2f} —Å–µ–∫.\")\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    texts = [item['text'] for item in examples['data']]\n",
    "    tokenized_inputs = tokenizer(texts, truncation=True, is_split_into_words=False)\n",
    "    labels = []\n",
    "    for i in range(len(texts)):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = [-100] * len(word_ids)\n",
    "        if examples['annotations'][i] and examples['annotations'][i][0].get('result'):\n",
    "            entities = examples['annotations'][i][0]['result']\n",
    "        else:\n",
    "            entities = []\n",
    "        for entity in entities:\n",
    "            if 'value' not in entity: continue\n",
    "            value = entity['value']\n",
    "            start_char, end_char, label = value['start'], value['end'], value['labels'][0]\n",
    "            token_start_index = tokenized_inputs.char_to_token(i, start_char)\n",
    "            token_end_index = tokenized_inputs.char_to_token(i, end_char - 1)\n",
    "            if token_start_index is not None and token_end_index is not None and f\"B-{label}\" in label2id:\n",
    "                label_ids[token_start_index] = label2id[f\"B-{label}\"]\n",
    "                for t_idx in range(token_start_index + 1, token_end_index + 1):\n",
    "                    label_ids[t_idx] = label2id[f\"I-{label}\"]\n",
    "        for j, word_id in enumerate(word_ids):\n",
    "            if label_ids[j] == -100 and word_id is not None:\n",
    "                 label_ids[j] = label2id[\"O\"]\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "print(\"\\n--- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ ---\")\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True, desc=\"Running tokenizer\")\n",
    "print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ ---\")\n",
    "start_time = time.time()\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "end_time = time.time()\n",
    "print(f\"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {end_time - start_time:.2f} —Å–µ–∫.\")\n",
    "\n",
    "num_params = model.num_parameters()\n",
    "num_params_trainable = model.num_parameters(only_trainable=True)\n",
    "model_size_mb = num_params * 4 / (1024**2)\n",
    "print(f\"\\n--- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ ---\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {num_params / 1_000_000:.2f} M\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {num_params_trainable / 1_000_000:.2f} M\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç–∏ (float32): {model_size_mb:.2f} MB\")\n",
    "print(\"---------------------------\\n\")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [[id2label[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[id2label[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    f1 = seqeval.metrics.f1_score(true_labels, true_predictions, average=\"macro\")\n",
    "    precision = seqeval.metrics.precision_score(true_labels, true_predictions, average=\"macro\")\n",
    "    recall = seqeval.metrics.recall_score(true_labels, true_predictions, average=\"macro\")\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=25,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    \n",
    "    eval_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",      \n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- –†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ—Å–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ ---\")\n",
    "\n",
    "manual_weights = {\n",
    "    \"O\": 0.5,\n",
    "    \n",
    "    \"B-LOCATION\": 1.0,\n",
    "    \"I-LOCATION\": 1.0,\n",
    "\n",
    "\n",
    "    \"B-WORK_OF_ART\": 1.0,\n",
    "    \"I-WORK_OF_ART\": 1.0,\n",
    "\n",
    "    \"B-AUTHOR\": 1.0,\n",
    "    \"I-AUTHOR\": 1.0,\n",
    "    \"B-PERSON\": 1.5,\n",
    "    \"I-PERSON\": 1.5,\n",
    "    \"B-CHARACTER\": 1.0,\n",
    "    \"I-CHARACTER\": 1.0,\n",
    "}\n",
    "\n",
    "num_labels = len(label2id)\n",
    "class_weights = torch.zeros(num_labels)\n",
    "\n",
    "for i in range(num_labels):\n",
    "    label_name = id2label[i]\n",
    "    class_weights[i] = manual_weights.get(label_name, 1.0)\n",
    "\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "print(\"\\n–ò—Ç–æ–≥–æ–≤—ã–µ —Ä—É—á–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å:\")\n",
    "for i in range(num_labels):\n",
    "    print(f\"  {id2label[i]}: {class_weights[i]:.4f}\")\n",
    "print(\"----------------------------------\\n\")\n",
    "\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"\\n--- –ù–∞—á–∏–Ω–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ---\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/best_model\")\n",
    "print(f\"\\n–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {OUTPUT_DIR}/best_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bcfe75",
   "metadata": {},
   "source": [
    "## –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54dd6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../data/my-chgk-ner-model-v1/best_model\" \n",
    "TEST_FILE = \"test_data.json\"\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {'cuda' if device == 0 else 'cpu'}\")\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...\")\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=MODEL_PATH,\n",
    "    tokenizer=MODEL_PATH,\n",
    "    aggregation_strategy=\"simple\", \n",
    "    device=device\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "with open(TEST_FILE, 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞.\")\n",
    "\n",
    "all_true_tags = []\n",
    "all_pred_tags = []\n",
    "\n",
    "print(\"\\n–ù–∞—á–∏–Ω–∞–µ–º –æ—Ü–µ–Ω–∫—É –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...\")\n",
    "\n",
    "for example in tqdm(test_data, desc=\"–û—Ü–µ–Ω–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤\"):\n",
    "    text = example['data']['text']\n",
    "    true_annotations = example['annotations'][0].get('result', [])\n",
    "\n",
    "    tokenized_inputs = tokenizer(text, return_offsets_mapping=True, truncation=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n",
    "    offsets = tokenized_inputs[\"offset_mapping\"]\n",
    "\n",
    "    true_tags = ['O'] * len(tokens)\n",
    "    for annotation in true_annotations:\n",
    "        value = annotation['value']\n",
    "        start_char, end_char, label = value['start'], value['end'], value['labels'][0]\n",
    "\n",
    "        token_start_index = -1\n",
    "        token_end_index = -1\n",
    "\n",
    "        for i, (offset_start, offset_end) in enumerate(offsets):\n",
    "            if offset_start <= start_char < offset_end:\n",
    "                token_start_index = i\n",
    "            if offset_start < end_char <= offset_end:\n",
    "                token_end_index = i\n",
    "        \n",
    "        if token_start_index != -1 and token_end_index != -1:\n",
    "            true_tags[token_start_index] = f\"B-{label}\"\n",
    "            for i in range(token_start_index + 1, token_end_index + 1):\n",
    "                true_tags[i] = f\"I-{label}\"\n",
    "\n",
    "    pred_tags = ['O'] * len(tokens)\n",
    "    predictions = ner_pipeline(text)\n",
    "    \n",
    "    for entity in predictions:\n",
    "        start_char, end_char, label = entity['start'], entity['end'], entity['entity_group']\n",
    "        \n",
    "        token_start_index = -1\n",
    "        token_end_index = -1\n",
    "\n",
    "        for i, (offset_start, offset_end) in enumerate(offsets):\n",
    "            if offset_start <= start_char < offset_end:\n",
    "                token_start_index = i\n",
    "            if offset_start < end_char <= offset_end:\n",
    "                token_end_index = i\n",
    "\n",
    "        if token_start_index != -1 and token_end_index != -1:\n",
    "            pred_tags[token_start_index] = f\"B-{label}\"\n",
    "            for i in range(token_start_index + 1, token_end_index + 1):\n",
    "                pred_tags[i] = f\"I-{label}\"\n",
    "\n",
    "    final_true_tags = []\n",
    "    final_pred_tags = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in (tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token):\n",
    "            final_true_tags.append(true_tags[i])\n",
    "            final_pred_tags.append(pred_tags[i])\n",
    "\n",
    "    all_true_tags.append(final_true_tags)\n",
    "    all_pred_tags.append(final_pred_tags)\n",
    "\n",
    "print(\"\\n–û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–∞—Å—á–µ—Ç –∏—Ç–æ–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫...\")\n",
    "report = classification_report(all_true_tags, all_pred_tags, digits=4)\n",
    "\n",
    "print(\"\\n--- –û—Ç—á–µ—Ç –ø–æ –∫–∞—á–µ—Å—Ç–≤—É NER-–º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ ---\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
